{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.DQN import DQNAgent\n",
    "from models.PPO import PPOAgent\n",
    "from models.DDPG import DDPGAgent\n",
    "from mpl_toolkits import mplot3d\n",
    "from core.visuals import *\n",
    "from models.reward_functions import *\n",
    "from tensorflow.keras.layers import *\n",
    "from core.stochastic_similarity import similarity\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "dqn_agent = DQNAgent(env, Identity, RMSprop(1e-4, decay=0.99), model_name='lunarlander', batch_size=32, warmup=0, validation_logging=5,validation_episodes=5,layers=[128,128],activation='relu',epsilon_min=0.05,epsilon_decay=0.999,tau=0.01,reg=0.01,gamma=0.999)\n",
    "dqn_agent.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, observations, actions = run_model(env, 100, dqn_agent.critic.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "dqn_shap_agent = DQNAgent(env, dqn_shap, RMSprop(1e-4, decay=0.99), model_name='lunarlander', batch_size=32, warmup=0, validation_logging=5,validation_episodes=5,layers=[128,128],activation='relu',epsilon_min=0.05,epsilon_decay=0.999,tau=0.01,reg=0.01,gamma=0.999)\n",
    "dqn_agent.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plot = action_space_contour(env, i, j, dqn_agent.critic.batch_predict, discrete=True, figsize=(8, 6), dpi=150)\n",
    "        plt.title(\"Shapley Reward Policy\")\n",
    "        plt.savefig(f\"dqn_shap_plots/obs{i}{j}.png\")\n",
    "        plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cont = gym.make(\"LunarLanderContinuous-v2\")\n",
    "ddpg_agent = DDPGAgent(env_cont, Identity, critic_optimizer=RMSprop(5e-3, decay=0.75),actor_optimizer=RMSprop(5e-3, decay=0.99),model_name='reloading',batch_size=256,warmup=0,actor_layers=[128,256],critic_layers=[128,256],actor_reg=0.01,critic_reg=0.01,activation=ELU,\n",
    "    epsilon_min=0.01,tau=0.01,gamma=0.999,validation_logging=10,validation_episodes=5)\n",
    "ddpg_agent.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_model(env_cont, 10, ddpg_agent.actor.predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
