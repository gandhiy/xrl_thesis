{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GaussianNoise, Input, concatenate\n",
    "from keras.layers import BatchNormalization, Flatten, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from core.replay_experience import ReplayMemory, Transition\n",
    "from models.reward_functions import Identity\n",
    "\n",
    "\n",
    "from pdb import set_trace as debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, obs_shape, act_shape, *args):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.act_shape = act_shape\n",
    "            \n",
    "    \n",
    "        \n",
    "    def __init_model__(self, model_params = [256, 128, 64]):\n",
    "        \n",
    "        inp_1 = Input((self.obs_shape))\n",
    "        inp_2 = Input((self.act_shape))\n",
    "        \n",
    "        x = Dense(model_params[0], activation='relu')(inp_1)\n",
    "        x = concatenate([x, inp_2])\n",
    "        for m in model_params[1:]:\n",
    "            x = Dense(m, activation='relu')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "        out = Dense(1, activation='linear', kernel_initializer=RandomUniform())(x)\n",
    "        self.model = Model([inp_1, inp_2], out)\n",
    "        \n",
    "    def __build_opt__(self, lr, b1, b2):\n",
    "        return Adam(\n",
    "            learning_rate=lr,\n",
    "            beta_1 = b1,\n",
    "            beta_2 = b2,\n",
    "            clipvalue=0.5\n",
    "        )\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.__init_model__()\n",
    "    \n",
    "    \n",
    "    def build_opt(self, learning_rate, beta_1, beta_2):\n",
    "        self.model.compile(optimizer=self.__build_opt__(learning_rate, beta_1, beta_2), loss='mse')\n",
    "   \n",
    "    \n",
    "    def predict(self, st, at):\n",
    "        if(len(st.shape) < 2):\n",
    "            assert len([st]) == len(at), 'mismatch between number of samples'\n",
    "            return self.model.predict([[st], at])\n",
    "        else:\n",
    "            assert len(st) == len(at), 'mistmatch between number of samples'\n",
    "            return self.model.predict([st, at])\n",
    "    \n",
    "    def transfer_weights(self, model, tau):\n",
    "        self.model.set_weights(\n",
    "            [tau*l1 + (1-tau)*l2 for l1, l2 in zip(self.model.get_weights(), model.get_weights())]\n",
    "        )\n",
    "\n",
    "        \n",
    "class Actor:\n",
    "    def __init__(self, obs_shape, act_shape, act_range):\n",
    "        self.obs_shape = obs_shape\n",
    "        \n",
    "        assert len(act_shape) < 2, \"Only Box environment allowed\"\n",
    "\n",
    "        self.act_shape = act_shape[0] \n",
    "        self.act_range = act_range\n",
    "        \n",
    "    def __init_model__(self, model_params=[256, 128, 64]):\n",
    "        inp = Input((self.obs_shape))\n",
    "        x = Dense(model_params[0], activation='relu')(inp)\n",
    "        for m in model_params[1:]:\n",
    "            x = Dense(m, activation='relu')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "        \n",
    "        x = GaussianNoise(1.0)(x)\n",
    "        # puts action out vals between 0 and 1\n",
    "        out = Dense(self.act_shape, activation='tanh', kernel_initializer=RandomUniform())(x)\n",
    "        \n",
    "        # set to the correct range\n",
    "        out = Lambda(lambda i: i * self.act_range)(out)\n",
    "        self.model= Model(inp, out)\n",
    "        \n",
    "    def __build_opt__(self, lr, b1, b2):\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt = Adam(learning_rate=lr, beta_1=b1, beta_2=b2)\n",
    "        \n",
    "        # build function to apply gradients to actor model\n",
    "        act_grads = K.placeholder(shape=(None, self.act_shape))\n",
    "#         clipped_grads = K.clip(act_grads, -0.5, 0.5)\n",
    "#         mean_grad = K.sum(clipped_grads, axis=0)\n",
    "        update_params = tf.gradients(self.model.output, self.model.trainable_weights, -clipped_grads)\n",
    "        grads = zip(update_params, self.model.trainable_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return K.function(\n",
    "            inputs=[self.model.input, act_grads], outputs=[mean_grad],\n",
    "            updates=[tf.train.AdamOptimizer(learning_rate=lr, beta1=b1, beta2=b2).apply_gradients(grads)][1:]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def init_model(self):\n",
    "        self.__init_model__()\n",
    "        \n",
    "    def build_opt(self, learning_rate, beta_1, beta_2):\n",
    "        return self.__build_opt__(learning_rate, beta_1, beta_2)\n",
    "        \n",
    "    def predict(self, st):\n",
    "        if(len(st.shape) < 2):\n",
    "            return self.model.predict(np.expand_dims(st, axis=0))\n",
    "        else:\n",
    "            return self.model.predict(st)\n",
    "        \n",
    "    def transfer_weights(self, model, tau):\n",
    "        self.model.set_weights(\n",
    "            [tau*l1 + (1-tau)*l2 for l1, l2 in zip(self.model.get_weights(), model.get_weights())]\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(\n",
    "        self, env, reward_class, batch_size=256, memory_size=1028, gamma=0.95, epsilon = 1.0, \n",
    "        epsilon_min=0.01, epsilon_decay=0.995, exploration_fraction=0.1, update_timesteps=50, \n",
    "        tau=0.01, learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.99, logger_steps = 500,\n",
    "        learning_starts = 500, render=False\n",
    "    ):\n",
    "        # learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.tau = tau\n",
    "        self.update_timesteps = update_timesteps\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.reward_function = reward_class().reward_func\n",
    "        self.learning_starts = learning_starts\n",
    "        \n",
    "        # environment parameters\n",
    "        self.env = env\n",
    "        self.render = render\n",
    "        \n",
    "        # setup models and optimizers\n",
    "        self.behavior_q = Critic(self.env.observation_space.shape, self.env.action_space.shape)\n",
    "        self.behavior_q.init_model()\n",
    "        self.behavior_q.build_opt(self.learning_rate, self.beta_1, self.beta_2)\n",
    "        \n",
    "        self.target_q = Critic(self.env.observation_space.shape, self.env.action_space.shape)\n",
    "        self.target_q.init_model()\n",
    "\n",
    "        # self.target_q.build_opt(self.learning_rate, self.beta_1, self.beta_2) not entirely necessary since we'll be updating the weights heuristically\n",
    "        \n",
    "        self.behavior_pi = Actor(self.env.observation_space.shape, self.env.action_space.shape, self.env.action_space.high)\n",
    "        self.behavior_pi.init_model()\n",
    "        self.behavior_pi_AdamOpt = self.behavior_pi.build_opt(self.learning_rate, self.beta_1, self.beta_2)\n",
    "        \n",
    "        self.target_pi = Actor(self.env.observation_space.shape, self.env.action_space.shape, self.env.action_space.high)\n",
    "        self.target_pi.init_model()\n",
    "        self.transfer_weights()\n",
    "        \n",
    "        # logging parameters\n",
    "        self.logging_step = logger_steps\n",
    "        self._current_timestep = 0\n",
    "        self._num_episodes = 1\n",
    "        self._eps_rew_list = []\n",
    "        self._mean_eps_rew = 0\n",
    "        self._eps_rew = 0\n",
    "        \n",
    "        self.__parameter_dict = None\n",
    "        \n",
    "    def update_dictionary(self):\n",
    "        self.__parameter_dict = {key:value for key, value in self.__dict__.items() if not key.startswith('__') and not callable(key)}\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def _finished_episode(self):\n",
    "        # logging reward from environment not from shap\n",
    "        self._num_episodes += 1\n",
    "        self._eps_rew_list.append(self._eps_rew)\n",
    "        self._mean_eps_rew = sum(self._eps_rew_list)/len(self._eps_rew_list)\n",
    "        self._eps_rew = 0\n",
    "        return self.env.reset()\n",
    "        \n",
    "    def act(self, st):\n",
    "        \n",
    "        if np.random.rand() <=self.epsilon:\n",
    "            at = self.env.action_space.sample()\n",
    "        else:\n",
    "            at = self.behavior_pi.predict(st)[0]\n",
    "\n",
    "        \n",
    "        snext, rt, done, _ = self.env.step(at)\n",
    "        if self.render:\n",
    "            self.env.render()\n",
    "        \n",
    "        if done:\n",
    "            snext = self._finished_episode()\n",
    "            \n",
    "        self._eps_rew += rt\n",
    "        self.memory.push(st, at, snext, done, rt)\n",
    "        return snext\n",
    "        \n",
    "\n",
    "    def transfer_weights(self):\n",
    "        self.target_q.transfer_weights(self.behavior_q.model, self.tau)\n",
    "        self.target_pi.transfer_weights(self.behavior_pi.model, self.tau)    \n",
    "\n",
    "        \n",
    "    \n",
    "    def update_on_batch(self):        \n",
    "        \"\"\"\n",
    "        primed => target model\n",
    "        non-primed => behavior model\n",
    "        y_j = r_j = \\gamma*Q'(s_{j+1}, \\mu'(s_{j+1} | \\theta^{\\mu'}) | \\theta^{Q'})\n",
    "        L_Q = \\frac{1}{N} \\sum_j (y_j - Q(s_j, a_j | \\theta^Q))^2 ==> squared bellman update\n",
    "        \\div_{\\theta^{\\mu}} J = \\frac{1}{N} \\sum_j \\div_a Q(s_j, \\mu(s_j | \\theta^{\\mu}) | \\theta^Q)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch = Transition(*zip(*self.memory.sample(self.batch_size)))\n",
    "        \n",
    "        states = np.array(batch.state)\n",
    "        actions = np.array(batch.action)\n",
    "        states_tp1 = np.array(batch.next_state)\n",
    "        actions_tp1 = self.target_pi.model.predict(states_tp1)\n",
    "            \n",
    "        #### CRITIC UPDATE ####\n",
    "        mask = np.ones(self.batch_size)*([not l for l in batch.done])\n",
    "        mask = mask.reshape((-1, 1))\n",
    "\n",
    "        y = self.target_q.model.predict([states_tp1, actions_tp1])\n",
    "        y *= self.gamma\n",
    "        y *= mask\n",
    "        \n",
    "        # apply shap updates here if desired\n",
    "        tmp, self.__parameter_dict = self.reward_function(batch, **self.__parameter_dict)\n",
    "        \n",
    "        y += np.array(tmp).reshape((-1, 1))\n",
    "        self.behavior_q.model.fit([states, actions], y, verbose=0)\n",
    "        \n",
    "        \n",
    "        #### ACTOR UPDATE ####\n",
    "        # get actions from actor model\n",
    "        acts = self.behavior_pi.predict(states)\n",
    "        \n",
    "        # get gradients of critic model wrt the actions taken by the actor model\n",
    "        get_action_grads = K.function(\n",
    "            [self.behavior_q.model.input[0], self.behavior_q.model.input[1]], \n",
    "            K.gradients(\n",
    "                self.behavior_q.model.output, [self.behavior_q.model.input[1]]\n",
    "            ))\n",
    "        \n",
    "        action_grads = get_action_grads([states, acts])\n",
    "        \n",
    "        # apply gradients \n",
    "        self.behavior_pi_AdamOpt([batch.state, np.array(action_grads).reshape(-1, self.env.action_space.shape[0])])\n",
    "        \n",
    "        \n",
    "            \n",
    "    def learn(self, total_timesteps):\n",
    "        st = self.env.reset()\n",
    "        assert self.learning_starts < total_timesteps\n",
    "        \n",
    "        for tt in range(total_timesteps):\n",
    "            \n",
    "            self.update_dictionary()\n",
    "            self._current_timestep = tt\n",
    "            st = self.act(st)\n",
    "            \n",
    "            if(self.memory.can_sample(self.batch_size) and tt > self.learning_starts):\n",
    "                self.update_on_batch()   \n",
    "                if (tt+1)%self.update_timesteps == 0:\n",
    "                    self.transfer_weights()\n",
    "                    \n",
    "            \n",
    "            if (tt+1)%self.logging_step == 0:\n",
    "                print(\n",
    "                    f\"Episodes: {self._num_episodes} \\t Average Reward {self._mean_eps_rew:.4f}\",\n",
    "                    f\"\\t Timesteps: {tt+1:.1f}\"\n",
    "                )\n",
    "                    \n",
    "            if self.epsilon > self.epsilon_min and tt < (self.exploration_fraction*total_timesteps):\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "                        \n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashgandhi/Documents/xrl_thesis/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env._max_episode_steps = 256\n",
    "model = DDPGAgent(env, Identity, learning_starts=0, logger_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 1 \t Average Reward 0.0000 \t Timesteps: 200.0\n",
      "Episodes: 2 \t Average Reward -4.9249 \t Timesteps: 400.0\n",
      "Episodes: 3 \t Average Reward -4.1968 \t Timesteps: 600.0\n",
      "Episodes: 4 \t Average Reward -4.0091 \t Timesteps: 800.0\n",
      "Episodes: 4 \t Average Reward -4.0091 \t Timesteps: 1000.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5a4ca07cf565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-840751fe0b1a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_timesteps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-840751fe0b1a>\u001b[0m in \u001b[0;36mupdate_on_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             K.gradients(\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             ))\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3022\u001b[0m     \"\"\"\n\u001b[1;32m   3023\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0mstop_gradient_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     reachable_to_ops, pending_count, loop_state = _PendingCount(\n\u001b[0;32m--> 617\u001b[0;31m         to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Iterate over the collected ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_PendingCount\u001b[0;34m(to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;31m# Mark reachable ops from from_ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mreached_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m   \u001b[0m_MarkReachedOps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreached_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;31m# X in reached_ops iff X is reachable from from_ops by a path of zero or more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[0;31m# backpropagatable tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MarkReachedOps\u001b[0;34m(from_ops, reached_ops, func_graphs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreached_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0mreached_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_IsBackpropagatable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m           \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Consumers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
